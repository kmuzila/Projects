{"cells":[{"cell_type":"markdown","source":["#Objective"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"90427fe0-b31f-493e-ad19-292a31643253"}}},{"cell_type":"markdown","source":["#Notebook Initialization"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3c724a43-5463-4a4f-aeb0-232028f49412"}}},{"cell_type":"markdown","source":["##Import Packages"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"da2e947b-e23a-4315-a962-d662b487ba60"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col,isnan,when,count,lit, to_date,lpad,date_format,rpad,regexp_replace,concat,to_utc_timestamp,to_timestamp, countDistinct,unix_timestamp, row_number, when\nfrom pyspark.sql.types import IntegerType,BooleanType,DateType,StringType,TimestampType\nfrom pyspark.sql import DataFrameNaFunctions\nfrom pyspark import StorageLevel\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pytz import timezone\nimport datetime\nfrom pyspark.ml import Pipeline, PipelineModel\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder, StandardScaler, PCA, VectorSlicer, Imputer\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql.functions import percent_rank\nfrom pyspark.sql import Window\nfrom pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier, DecisionTreeClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.sql import functions as f"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Import Packages","showTitle":false,"inputWidgets":{},"nuid":"eb5f32a1-39f8-4804-9b43-0a436ad9d67d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["##Cloud Storage Parameters"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e7922ea-536f-4a49-a76f-4e2f5487dd9c"}}},{"cell_type":"code","source":["blob_container = \"tm30container\" # The name of your container created in https://portal.azure.com\nstorage_account = \"w261tm30\" # The name of your Storage account created in https://portal.azure.com\nsecret_scope = \"w261tm30\" # The name of the scope created in your local computer using the Databricks CLI\nsecret_key = \"tm30key\" # The name of the secret key created in your local computer using the Databricks CLI \nblob_url = f\"wasbs://{blob_container}@{storage_account}.blob.core.windows.net\"\nmount_path = \"/mnt/mids-w261\"\n\ntest_pq = spark.read.parquet(f\"{blob_url}/2022-03-24_data_chkpt_PQ_full\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82f32347-9997-48da-942e-5ba3025cb9f5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["##Define Functions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f4ef8332-5fd7-4d3d-b86b-43cc45fa14c3"}}},{"cell_type":"code","source":["def data_pull(df, time_window = 'full', date_col='FLIGHT_UTC_DATE'):\n    \"\"\"Pull processed dataset\"\"\"\n    if time_window == '2019':\n        df = df.filter(f.year(col(date_col)) == 2019)\n    elif time_window == '2018':\n        df = df.filter(f.year(col(date_col)) == 2018)\n    elif time_window == '2017':\n        df = df.filter(f.year(col(date_col)) == 2017)\n    elif time_window == '2016':\n        df = df.filter(f.year(col(date_col)) == 2016) \n    \n    #The commands below are for 2015 data\n    elif time_window == '6m':\n        df = df.filter(col(date_col) < \"2015-07-01T00:00:00.000\")  \n    elif time_window == '3m':\n        df = df.filter(col(date_col) < \"2015-04-01T00:00:00.000\")\n        #comment this out if it takes too long\n    \n    print(f'{df.count():,} total records imported for the {time_window} dataset')\n    return df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e34abfa-301e-468b-aba6-1163138d439e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def pre_pipeline(index_cols, cont_cols, cat_cols, pred_cols):\n    ''' This function creates a pre-processed pipeline to be used to prepare for crossfold validation and model training\n    '''\n    pre_pipeline = None\n    \n    #Convert string to index\n    indexer = StringIndexer(inputCols=cat_cols, outputCols=[c+\"_idx\" for c in cat_cols]).setHandleInvalid(\"keep\")\n\n    #Convert categorical columns to index\n    encoder = OneHotEncoder(inputCols=[c+\"_idx\" for c in cat_cols], outputCols= [c+\"_OHE\" for c in cat_cols])\n    \n    #Vector assembler for categorical\n    assembler_cat = VectorAssembler(inputCols= [x+\"_OHE\" for x in cat_cols], outputCol=\"cat_features\")\n        \n    assembler_lab = StringIndexer(inputCol='DEP_DEL15', outputCol=\"label\")\n        \n    pre_pipeline = Pipeline(stages=[indexer, encoder, assembler_cat, assembler_lab])\n    \n    return pre_pipeline"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"31375348-26b8-4619-8b56-8db5085d4c17"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def scaled_pipeline(model, param_grid):\n    ''' This function creates a scaled processed and scaled pipeline to be used to train models.\n        Parameters:\n            model:    lr = Logistic Regression;\n                      rf = Random Forest \n                      dt = Decision Trees\n        Returns: a pipeline model\n    '''\n    pipeline_model = None\n    \n    #Ensure continuous variables have values\n    imputer = Imputer(inputCols=cont_cols, outputCols=cont_cols)\n    \n    #Assemble cont variables\n    assembler_num = VectorAssembler(inputCols=cont_cols, outputCol=\"scale_nums\")\n    \n    #Scale the values\n    scaler = StandardScaler(inputCol=\"scale_nums\", outputCol=\"scaledFeatures\", withStd=True, withMean=True)\n\n    #Vector assembler combined\n    assembler = VectorAssembler(inputCols=[\"scaledFeatures\", \"cat_features\"], outputCol=\"features\")\n    \n    #Models for the pipeline\n    if model == 'lr':\n        max_iter, reg_param, ela = param_grid.values()\n        \n        class_model = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter = max_iter, regParam = reg_param, elasticNetParam = ela)\n        \n    elif model == 'rf':\n        num_trees, depth, criterion, features = param_grid.values()\n        \n        class_model = RandomForestClassifier(featuresCol = 'features', labelCol = 'label', numTrees = num_trees, maxDepth = depth, impurity = criterion, featureSubsetStrategy = features, maxMemoryInMB = 512, minInfoGain =  1e-5, cacheNodeIds = True)\n        \n    elif model == 'dt':\n        md, mb, info = param_grid.values()\n    \n        class_model = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = md, maxBins = mb, impurity = info, maxMemoryInMB = 512, minInfoGain =  1e-5, cacheNodeIds = True)\n        \n    pipeline_model = Pipeline(stages=[imputer, assembler_num, scaler, assembler, class_model])\n    \n    return pipeline_model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ca1651af-5164-4b91-9efc-07c30f87ed87"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def custom_CV(df_rank, pre_pipeline, class_model, sample, kfolds):\n\n    #Create evaluation metric lists\n    f_5_score_list_CV_average = []\n\n    #Logistic Model Lists\n    maxiteration = []\n    regulationparameter = []\n    elasticnet = []\n    \n    #Decision Tree Model Lists\n    maxdepth = []\n    maxbins = []\n    infotype = []\n    \n    #Random Forest Model Lists\n    numtrees = []\n    depthtree = []\n    criterion_type = []\n    feature_subset = []\n    \n    #Preprocess Pipleine\n    fit_df = pre_pipeline.fit(df_rank)\n    transform_df = fit_df.transform(df_rank).persist(StorageLevel.MEMORY_AND_DISK)\n    \n    if class_model == 'lr':\n        maxiter = [10, 100]\n        regulation_param = [0.01, 0.5, 2.0]\n        elastic = [0.0, 0.5, 1.0]\n        \n        for max_iter in maxiter:\n            for reg_param in regulation_param:\n                for ela in elastic:\n                    \n                    # Create Time Splits\n                    splits = 1.0/(kfolds + 1)\n                    cutoff = splits\n                    \n                    f_5_score_list = []\n                    param_grid = {'maxIter':max_iter, 'regParam': reg_param, 'elasticNetParam':ela}\n                    \n                    scaled_pipelines = scaled_pipeline('lr', param_grid)\n                    \n                    for split in range(kfolds):\n                        \n                        train_df = transform_df.where(f\"rank <= {cutoff}\").cache()\n                        test_df = transform_df.where(f\"rank > {cutoff} and rank <= {cutoff+splits}\").cache()\n                        cutoff += splits\n                        \n                        if sample == 'down':\n                            train_df = downsample(train_df)\n                            \n                        #Generate model \n                        model = scaled_pipelines.fit(train_df)\n                        predict = model.transform(test_df)\n                        \n#                       Calculate evaluation metrics\n                        evaluatorf_5 = MulticlassClassificationEvaluator(metricName='fMeasureByLabel', metricLabel=1, beta=0.5)\n                        f_5 = evaluatorf_5.evaluate(predict)\n                        f_5_score_list.append(f_5)\n                    \n                    f_score_avg = np.mean(f_5_score_list)\n                    \n                    f_5_score_list_CV_average.append(f_score_avg)\n                    maxiteration.append(max_iter)\n                    regulationparameter.append(reg_param)\n                    elasticnet.append(ela)\n\n                        \n                    print('F Score: {:3f}\\nParam Grid: {}'.format(f_score_avg, param_grid.items()))\n                    \n                    \n        Eval_df = pd.DataFrame()\n        Eval_df['F_0.5 Score'] = f_5_score_list_CV_average\n        Eval_df['Max Iterations'] = maxiteration\n        Eval_df['Regulation Parameter'] = regulationparameter\n        Eval_df['Elastic Net'] = elasticnet\n        \n    elif class_model == 'dt':\n        max_depth = [5, 10, 20]\n        max_bins = [6168]\n        info_type = ['gini']\n        \n        for md in max_depth:\n            for mb in max_bins:\n                for it in info_type:\n                    \n                    # Create Time Splits\n                    splits = 1.0/(kfolds + 1)\n                    cutoff = splits\n                    \n                    f_5_score_list = []\n                    param_grid = {'max_depth':md, 'max_bins': mb, 'info_type':it}\n                    \n                    scaled_pipelines = scaled_pipeline('dt', param_grid)\n                    \n                    for split in range(kfolds):\n                        \n                        train_df = transform_df.where(f\"rank <= {cutoff}\").cache()\n                        test_df = transform_df.where(f\"rank > {cutoff} and rank <= {cutoff+splits}\").cache()\n                        cutoff += splits\n                        \n                        if sample == 'down':\n                            train_df = downsample(train_df)\n                            \n                        #Generate model \n                        model = scaled_pipelines.fit(train_df)\n                        predict = model.transform(test_df)\n                        \n#                       Calculate evaluation metrics\n                        evaluatorf_5 = MulticlassClassificationEvaluator(metricName='fMeasureByLabel', metricLabel=1, beta=0.5)\n                        f_5 = evaluatorf_5.evaluate(predict)\n                        f_5_score_list.append(f_5)\n                    \n                    f_score_avg = np.mean(f_5_score_list)\n                    \n                    f_5_score_list_CV_average.append(f_score_avg)\n                    maxdepth.append(md)\n                    maxbins.append(mb)\n                    infotype.append(it)\n                        \n                    print('F Score: {:3f}\\nParam Grid: {}'.format(f_score_avg, param_grid.items()))\n                    \n                    \n        Eval_df = pd.DataFrame()\n        Eval_df['F_0.5 Score'] = f_5_score_list_CV_average\n        Eval_df['Max Depth'] = maxdepth\n        Eval_df['Max Bins'] = maxbins\n        Eval_df['Info Type'] = infotype\n\n    elif class_model == 'rf':\n        num_trees = [10, 20, 50]\n        depth = [3, 5, 10]\n        criterion = ['gini']\n        feature_subsetstrat = ['sqrt', 'all']\n        \n        for nt in num_trees:\n            for d in depth:\n                for crit in criterion:\n                    for feature in feature_subsetstrat:\n                    \n                        # Create Time Splits\n                        splits = 1.0/(kfolds + 1)\n                        cutoff = splits\n\n                        f_5_score_list = []\n                        param_grid = {'num_trees':nt, 'depth': d, 'criterion':crit, 'substrategy':feature}\n\n                        scaled_pipelines = scaled_pipeline('rf', param_grid)\n\n                        for split in range(kfolds):\n\n                            train_df = transform_df.where(f\"rank <= {cutoff}\").cache()\n                            test_df = transform_df.where(f\"rank > {cutoff} and rank <= {cutoff+splits}\").cache()\n                            cutoff += splits\n\n                            if sample == 'down':\n                                train_df = downsample(train_df)\n\n                            #Generate model \n                            model = scaled_pipelines.fit(train_df)\n                            predict = model.transform(test_df)\n\n    #                       Calculate evaluation metrics\n                            evaluatorf_5 = MulticlassClassificationEvaluator(metricName='fMeasureByLabel', metricLabel=1, beta=0.5)\n                            f_5 = evaluatorf_5.evaluate(predict)\n                            f_5_score_list.append(f_5)\n\n                        f_score_avg = np.mean(f_5_score_list)\n\n                        f_5_score_list_CV_average.append(f_score_avg)\n                        numtrees.append(nt)\n                        depthtree.append(d)\n                        criterion_type.append(crit)\n                        feature_subset.append(feature)\n\n                        print('F Score: {:3f}\\nParam Grid: {}'.format(f_score_avg, param_grid.items()))\n                    \n                    \n        Eval_df = pd.DataFrame()\n        Eval_df['F_0.5 Score'] = f_5_score_list_CV_average\n        Eval_df['Tree Number'] = numtrees\n        Eval_df['Depth'] = depthtree\n        Eval_df['Info Type'] = criterion_type\n        Eval_df['Feature Substrategy'] = feature_subset\n        \n    transform_df.unpersist()\n    \n    return Eval_df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a6a3a554-5c25-4f96-a174-04bc2fe41c4c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def threshold_CV(df_rank, pre_pipeline, class_model, sample, kfolds):\n\n    #Create evaluation metric lists\n    f_5_score_list_CV_average = []\n    threshold_list = []\n    \n    #Logistic Model Lists\n    threshold = [.2, .3, .4, .6, .7, .8, .9]\n    \n    #Preprocess Pipleine\n    fit_df = pre_pipeline.fit(df_rank)\n    transform_df = fit_df.transform(df_rank).persist(StorageLevel.MEMORY_AND_DISK)\n    \n    if class_model == 'lr':\n        \n        for thresh in threshold:\n                    \n                    # Create Time Splits\n            splits = 1.0/(kfolds + 1)\n            cutoff = splits\n                    \n            f_5_score_list = []\n            param_grid = {'maxIter':10, 'regParam': 0, 'elasticNetParam':1, 'threshold':thresh}\n                    \n            scaled_pipelines = scaled_pipeline('lr', param_grid)\n                    \n            for split in range(kfolds):\n                        \n                train_df = transform_df.where(f\"rank <= {cutoff}\").cache()\n                test_df = transform_df.where(f\"rank > {cutoff} and rank <= {cutoff+splits}\").cache()\n                cutoff += splits\n                        \n                if sample == 'down':\n                    train_df = downsample(train_df)\n                            \n                        #Generate model \n                model = scaled_pipelines.fit(train_df)\n                predict = model.transform(test_df)\n                        \n#                       Calculate evaluation metrics\n                evaluatorf_5 = MulticlassClassificationEvaluator(metricName='fMeasureByLabel', metricLabel=1, beta=0.5)\n                f_5 = evaluatorf_5.evaluate(predict)\n                f_5_score_list.append(f_5)\n                    \n            f_score_avg = np.mean(f_5_score_list)\n                    \n            f_5_score_list_CV_average.append(f_score_avg)\n            threshold_list.append(thresh)\n                        \n            print('F Score: {:3f}\\nParam Grid: {}'.format(f_score_avg, param_grid.items()))\n                    \n                    \n        Eval_df = pd.DataFrame()\n        Eval_df['F_0.5 Score'] = f_5_score_list_CV_average\n        Eval_df['Threshhold'] = threshold_list\n        \n    transform_df.unpersist()\n    \n    return Eval_df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"25123479-66b1-4fbc-aa46-c1d24e0bc512"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def downsample(train_df):\n    '''Downsample minority class to balance classes. \n    Only works if delay count is less than on_time count (should be the case...)'''\n    \n    delay_count = train_df.filter(col(\"label\") == 1).count()\n    on_time_count = train_df.filter(col(\"label\") == 0).count()\n    \n    total_count = delay_count + on_time_count\n    delay_pct = delay_count / on_time_count\n    \n    train_delay = train_df.filter(col('label') == 1)\n    train_on_time = train_df.filter(col('label') == 0).sample(withReplacement=False, fraction = delay_pct, seed= 2022)\n    train_downsampled = train_delay.union(train_on_time)\n    \n    return train_downsampled"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"71ee9224-4149-41db-b911-2caf88c8bf70"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#Pipeline Initialization"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"866585bd-c1d7-4f01-804d-ae881d24d48b"}}},{"cell_type":"code","source":["#Last minute data curation...\ntest_pq = test_pq.na.replace('', None, 'wnd_type')\\\n            .na.replace('', None, 'ga1_cld')\\\n            .na.replace('', None, 'ga1_cov')\\\n            .withColumn('wnd_dir_angle',col('wnd_dir_angle').cast(IntegerType()))\\\n            .withColumn('ka1_temp', when(f.isnull('ka1_temp'), '0').when(f.col('ka1_temp') < 0, -1).otherwise('1'))\\\n            .withColumn('FLIGHT_ROUTE', concat(col('ORIGIN'),lit(\"-\"),col('DEST')))\n\ndf_2015_2018 = test_pq.filter(col('FLIGHT_UTC_DATE') < \"2019-01-01T00:00:00.000\")\n\ndf_6m = data_pull(test_pq, time_window='6m', date_col='FLIGHT_UTC_DATE')\n\ndf_2019 = data_pull(test_pq, time_window='2019', date_col='FLIGHT_UTC_DATE')\n\ndf_small_test = test_pq.filter(col('FLIGHT_UTC_DATE') < \"2015-02-01T00:00:00.000\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"550e7158-1318-4390-b251-df02550a0c80"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">3,365,522 total records imported for the 6m dataset\n8,539,842 total records imported for the 2019 dataset\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">3,365,522 total records imported for the 6m dataset\n8,539,842 total records imported for the 2019 dataset\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["(df_2015_2018.count(), len(df_2015_2018.columns))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e083fb41-02fd-434c-8cf5-8b232798e542"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[53]: (27959979, 100)</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[53]: (27959979, 100)</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#Create Baseline Models"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"332541dc-d6d6-43b4-8544-6398c75d561f"}}},{"cell_type":"code","source":["#select columns\nindex_cols = ['UNIQUE_ID','FLIGHT_UTC_DATE', 'rank']\ncat_cols = ['TIME_OF_DAY', 'MONTH', 'DAY_OF_WEEK', 'OP_UNIQUE_CARRIER', 'wnd_type', 'cig_ceil_is_qual', 'tmp_air_is_qual',  'slp_prs_is_qual', 'ga1_cov','ga1_cld', 'ga1_bs_ht_is_qual', 'wnd_spd_is_qual', 'ga1_cld_qual', 'dew_pnt_is_qual', 'ga1_cov_is_qual', 'aa1_is_qual', 'vis_dist_is_qual', 'ka1_temp', 'FLIGHT_ROUTE']\ncont_cols = ['ELEVATION', 'wnd_dir_angle', 'wnd_spd_rate', 'cig_ceil_ht', 'vis_dist', 'tmp_air', 'dew_pnt_tmp','slp_prs', 'aa1_prd_quant_hr', 'aa1_dp', 'ga1_bs_ht']\npred_cols = ['DEP_DEL15']\n\n#Modify to switch from test to scaled up model\ndf_model = df_2015_2018\n\n#Add rank to allow forx custom crossvalidation and windowing\ntrain_test_window = df_model.withColumn(\"rank\", percent_rank().over(Window.partitionBy().orderBy(\"FLIGHT_UTC_DATE\")))\n\n#Initialize Pre-processing Pipeline\npre_pipe = pre_pipeline(index_cols, cont_cols, cat_cols, pred_cols)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d94eba8e-df8f-455e-a733-33a4febd7734"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["##Logistic Regression"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"781971f2-2eb9-4497-93f7-c7b674e580cf"}}},{"cell_type":"code","source":["Eval_lr = custom_CV(train_test_window, pre_pipe, 'lr', 'down', 5)\nEval_lr"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2ad26afa-4128-40a7-b863-ef637c56d8fd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">F Score: 0.323845\nParam Grid: dict_items([(&#39;maxIter&#39;, 10), (&#39;regParam&#39;, 0.01), (&#39;elasticNetParam&#39;, 0.0)])\nF Score: 0.311193\nParam Grid: dict_items([(&#39;maxIter&#39;, 10), (&#39;regParam&#39;, 0.01), (&#39;elasticNetParam&#39;, 0.5)])\nF Score: 0.306506\nParam Grid: dict_items([(&#39;maxIter&#39;, 10), (&#39;regParam&#39;, 0.01), (&#39;elasticNetParam&#39;, 1.0)])\nF Score: 0.320416\nParam Grid: dict_items([(&#39;maxIter&#39;, 10), (&#39;regParam&#39;, 0.5), (&#39;elasticNetParam&#39;, 0.0)])\nF Score: 0.094082\nParam Grid: dict_items([(&#39;maxIter&#39;, 10), (&#39;regParam&#39;, 0.5), (&#39;elasticNetParam&#39;, 0.5)])\nF Score: 0.094082\nParam Grid: dict_items([(&#39;maxIter&#39;, 10), (&#39;regParam&#39;, 0.5), (&#39;elasticNetParam&#39;, 1.0)])\nF Score: 0.315576\nParam Grid: dict_items([(&#39;maxIter&#39;, 10), (&#39;regParam&#39;, 2.0), (&#39;elasticNetParam&#39;, 0.0)])\nF Score: 0.094082\nParam Grid: dict_items([(&#39;maxIter&#39;, 10), (&#39;regParam&#39;, 2.0), (&#39;elasticNetParam&#39;, 0.5)])\nF Score: 0.094082\nParam Grid: dict_items([(&#39;maxIter&#39;, 10), (&#39;regParam&#39;, 2.0), (&#39;elasticNetParam&#39;, 1.0)])\nF Score: 0.326902\nParam Grid: dict_items([(&#39;maxIter&#39;, 100), (&#39;regParam&#39;, 0.01), (&#39;elasticNetParam&#39;, 0.0)])\nF Score: 0.312312\nParam Grid: dict_items([(&#39;maxIter&#39;, 100), (&#39;regParam&#39;, 0.01), (&#39;elasticNetParam&#39;, 0.5)])\nF Score: 0.306830\nParam Grid: dict_items([(&#39;maxIter&#39;, 100), (&#39;regParam&#39;, 0.01), (&#39;elasticNetParam&#39;, 1.0)])\nF Score: 0.320402\nParam Grid: dict_items([(&#39;maxIter&#39;, 100), (&#39;regParam&#39;, 0.5), (&#39;elasticNetParam&#39;, 0.0)])\nF Score: 0.094082\nParam Grid: dict_items([(&#39;maxIter&#39;, 100), (&#39;regParam&#39;, 0.5), (&#39;elasticNetParam&#39;, 0.5)])\nF Score: 0.094082\nParam Grid: dict_items([(&#39;maxIter&#39;, 100), (&#39;regParam&#39;, 0.5), (&#39;elasticNetParam&#39;, 1.0)])\nF Score: 0.315590\nParam Grid: dict_items([(&#39;maxIter&#39;, 100), (&#39;regParam&#39;, 2.0), (&#39;elasticNetParam&#39;, 0.0)])\nF Score: 0.094082\nParam Grid: dict_items([(&#39;maxIter&#39;, 100), (&#39;regParam&#39;, 2.0), (&#39;elasticNetParam&#39;, 0.5)])\nF Score: 0.094082\nParam Grid: dict_items([(&#39;maxIter&#39;, 100), (&#39;regParam&#39;, 2.0), (&#39;elasticNetParam&#39;, 1.0)])\nOut[10]: </div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">F Score: 0.323845\nParam Grid: dict_items([(&#39;maxIter&#39;, 10), (&#39;regParam&#39;, 0.01), (&#39;elasticNetParam&#39;, 0.0)])\nF Score: 0.311193\nParam Grid: dict_items([(&#39;maxIter&#39;, 10), (&#39;regParam&#39;, 0.01), (&#39;elasticNetParam&#39;, 0.5)])\nF Score: 0.306506\nParam Grid: dict_items([(&#39;maxIter&#39;, 10), (&#39;regParam&#39;, 0.01), (&#39;elasticNetParam&#39;, 1.0)])\nF Score: 0.320416\nParam Grid: dict_items([(&#39;maxIter&#39;, 10), (&#39;regParam&#39;, 0.5), (&#39;elasticNetParam&#39;, 0.0)])\nF Score: 0.094082\nParam Grid: dict_items([(&#39;maxIter&#39;, 10), (&#39;regParam&#39;, 0.5), (&#39;elasticNetParam&#39;, 0.5)])\nF Score: 0.094082\nParam Grid: dict_items([(&#39;maxIter&#39;, 10), (&#39;regParam&#39;, 0.5), (&#39;elasticNetParam&#39;, 1.0)])\nF Score: 0.315576\nParam Grid: dict_items([(&#39;maxIter&#39;, 10), (&#39;regParam&#39;, 2.0), (&#39;elasticNetParam&#39;, 0.0)])\nF Score: 0.094082\nParam Grid: dict_items([(&#39;maxIter&#39;, 10), (&#39;regParam&#39;, 2.0), (&#39;elasticNetParam&#39;, 0.5)])\nF Score: 0.094082\nParam Grid: dict_items([(&#39;maxIter&#39;, 10), (&#39;regParam&#39;, 2.0), (&#39;elasticNetParam&#39;, 1.0)])\nF Score: 0.326902\nParam Grid: dict_items([(&#39;maxIter&#39;, 100), (&#39;regParam&#39;, 0.01), (&#39;elasticNetParam&#39;, 0.0)])\nF Score: 0.312312\nParam Grid: dict_items([(&#39;maxIter&#39;, 100), (&#39;regParam&#39;, 0.01), (&#39;elasticNetParam&#39;, 0.5)])\nF Score: 0.306830\nParam Grid: dict_items([(&#39;maxIter&#39;, 100), (&#39;regParam&#39;, 0.01), (&#39;elasticNetParam&#39;, 1.0)])\nF Score: 0.320402\nParam Grid: dict_items([(&#39;maxIter&#39;, 100), (&#39;regParam&#39;, 0.5), (&#39;elasticNetParam&#39;, 0.0)])\nF Score: 0.094082\nParam Grid: dict_items([(&#39;maxIter&#39;, 100), (&#39;regParam&#39;, 0.5), (&#39;elasticNetParam&#39;, 0.5)])\nF Score: 0.094082\nParam Grid: dict_items([(&#39;maxIter&#39;, 100), (&#39;regParam&#39;, 0.5), (&#39;elasticNetParam&#39;, 1.0)])\nF Score: 0.315590\nParam Grid: dict_items([(&#39;maxIter&#39;, 100), (&#39;regParam&#39;, 2.0), (&#39;elasticNetParam&#39;, 0.0)])\nF Score: 0.094082\nParam Grid: dict_items([(&#39;maxIter&#39;, 100), (&#39;regParam&#39;, 2.0), (&#39;elasticNetParam&#39;, 0.5)])\nF Score: 0.094082\nParam Grid: dict_items([(&#39;maxIter&#39;, 100), (&#39;regParam&#39;, 2.0), (&#39;elasticNetParam&#39;, 1.0)])\nOut[10]: </div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>F_0.5 Score</th>\n      <th>Max Iterations</th>\n      <th>Regulation Parameter</th>\n      <th>Elastic Net</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.323845</td>\n      <td>10</td>\n      <td>0.01</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.311193</td>\n      <td>10</td>\n      <td>0.01</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.306506</td>\n      <td>10</td>\n      <td>0.01</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.320416</td>\n      <td>10</td>\n      <td>0.50</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.094082</td>\n      <td>10</td>\n      <td>0.50</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.094082</td>\n      <td>10</td>\n      <td>0.50</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.315576</td>\n      <td>10</td>\n      <td>2.00</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.094082</td>\n      <td>10</td>\n      <td>2.00</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.094082</td>\n      <td>10</td>\n      <td>2.00</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.326902</td>\n      <td>100</td>\n      <td>0.01</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.312312</td>\n      <td>100</td>\n      <td>0.01</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.306830</td>\n      <td>100</td>\n      <td>0.01</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.320402</td>\n      <td>100</td>\n      <td>0.50</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.094082</td>\n      <td>100</td>\n      <td>0.50</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.094082</td>\n      <td>100</td>\n      <td>0.50</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.315590</td>\n      <td>100</td>\n      <td>2.00</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.094082</td>\n      <td>100</td>\n      <td>2.00</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.094082</td>\n      <td>100</td>\n      <td>2.00</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>F_0.5 Score</th>\n      <th>Max Iterations</th>\n      <th>Regulation Parameter</th>\n      <th>Elastic Net</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.323845</td>\n      <td>10</td>\n      <td>0.01</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.311193</td>\n      <td>10</td>\n      <td>0.01</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.306506</td>\n      <td>10</td>\n      <td>0.01</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.320416</td>\n      <td>10</td>\n      <td>0.50</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.094082</td>\n      <td>10</td>\n      <td>0.50</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.094082</td>\n      <td>10</td>\n      <td>0.50</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.315576</td>\n      <td>10</td>\n      <td>2.00</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.094082</td>\n      <td>10</td>\n      <td>2.00</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.094082</td>\n      <td>10</td>\n      <td>2.00</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.326902</td>\n      <td>100</td>\n      <td>0.01</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.312312</td>\n      <td>100</td>\n      <td>0.01</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.306830</td>\n      <td>100</td>\n      <td>0.01</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.320402</td>\n      <td>100</td>\n      <td>0.50</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.094082</td>\n      <td>100</td>\n      <td>0.50</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.094082</td>\n      <td>100</td>\n      <td>0.50</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.315590</td>\n      <td>100</td>\n      <td>2.00</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.094082</td>\n      <td>100</td>\n      <td>2.00</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.094082</td>\n      <td>100</td>\n      <td>2.00</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["##Decision Tree"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"32d92fe8-bde0-4daa-8c5f-1c887f9a0b3b"}}},{"cell_type":"code","source":["Eval_dt = custom_CV(train_test_window, pre_pipe, 'dt', 'down', 5)\nEval_dt"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"77e5f15c-0af1-4da2-b8f7-c25f8c174ab7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">F Score: 0.305253\nParam Grid: dict_items([(&#39;max_depth&#39;, 5), (&#39;max_bins&#39;, 6168), (&#39;info_type&#39;, &#39;gini&#39;)])\nF Score: 0.324010\nParam Grid: dict_items([(&#39;max_depth&#39;, 10), (&#39;max_bins&#39;, 6168), (&#39;info_type&#39;, &#39;gini&#39;)])\nF Score: 0.321351\nParam Grid: dict_items([(&#39;max_depth&#39;, 20), (&#39;max_bins&#39;, 6168), (&#39;info_type&#39;, &#39;gini&#39;)])\nOut[13]: </div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">F Score: 0.305253\nParam Grid: dict_items([(&#39;max_depth&#39;, 5), (&#39;max_bins&#39;, 6168), (&#39;info_type&#39;, &#39;gini&#39;)])\nF Score: 0.324010\nParam Grid: dict_items([(&#39;max_depth&#39;, 10), (&#39;max_bins&#39;, 6168), (&#39;info_type&#39;, &#39;gini&#39;)])\nF Score: 0.321351\nParam Grid: dict_items([(&#39;max_depth&#39;, 20), (&#39;max_bins&#39;, 6168), (&#39;info_type&#39;, &#39;gini&#39;)])\nOut[13]: </div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>F_0.5 Score</th>\n      <th>Max Depth</th>\n      <th>Max Bins</th>\n      <th>Info Type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.305253</td>\n      <td>5</td>\n      <td>6168</td>\n      <td>gini</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.324010</td>\n      <td>10</td>\n      <td>6168</td>\n      <td>gini</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.321351</td>\n      <td>20</td>\n      <td>6168</td>\n      <td>gini</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>F_0.5 Score</th>\n      <th>Max Depth</th>\n      <th>Max Bins</th>\n      <th>Info Type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.305253</td>\n      <td>5</td>\n      <td>6168</td>\n      <td>gini</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.324010</td>\n      <td>10</td>\n      <td>6168</td>\n      <td>gini</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.321351</td>\n      <td>20</td>\n      <td>6168</td>\n      <td>gini</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["##Random Forest"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8e7a680d-dd75-4dc0-a749-09d76ea17fff"}}},{"cell_type":"code","source":["Eval_rf = custom_CV(train_test_window, pre_pipe, 'rf', 'down', 5)\nEval_rf"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3cda7506-1a04-47a7-adb7-1c2125c37811"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">F Score: 0.279684\nParam Grid: dict_items([(&#39;num_trees&#39;, 10), (&#39;depth&#39;, 3), (&#39;criterion&#39;, &#39;gini&#39;), (&#39;substrategy&#39;, &#39;sqrt&#39;)])\nF Score: 0.288759\nParam Grid: dict_items([(&#39;num_trees&#39;, 10), (&#39;depth&#39;, 3), (&#39;criterion&#39;, &#39;gini&#39;), (&#39;substrategy&#39;, &#39;all&#39;)])\nF Score: 0.288417\nParam Grid: dict_items([(&#39;num_trees&#39;, 10), (&#39;depth&#39;, 5), (&#39;criterion&#39;, &#39;gini&#39;), (&#39;substrategy&#39;, &#39;sqrt&#39;)])\nF Score: 0.304559\nParam Grid: dict_items([(&#39;num_trees&#39;, 10), (&#39;depth&#39;, 5), (&#39;criterion&#39;, &#39;gini&#39;), (&#39;substrategy&#39;, &#39;all&#39;)])\nF Score: 0.296859\nParam Grid: dict_items([(&#39;num_trees&#39;, 10), (&#39;depth&#39;, 10), (&#39;criterion&#39;, &#39;gini&#39;), (&#39;substrategy&#39;, &#39;sqrt&#39;)])\nF Score: 0.323125\nParam Grid: dict_items([(&#39;num_trees&#39;, 10), (&#39;depth&#39;, 10), (&#39;criterion&#39;, &#39;gini&#39;), (&#39;substrategy&#39;, &#39;all&#39;)])\nF Score: 0.294743\nParam Grid: dict_items([(&#39;num_trees&#39;, 20), (&#39;depth&#39;, 3), (&#39;criterion&#39;, &#39;gini&#39;), (&#39;substrategy&#39;, &#39;sqrt&#39;)])\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">F Score: 0.279684\nParam Grid: dict_items([(&#39;num_trees&#39;, 10), (&#39;depth&#39;, 3), (&#39;criterion&#39;, &#39;gini&#39;), (&#39;substrategy&#39;, &#39;sqrt&#39;)])\nF Score: 0.288759\nParam Grid: dict_items([(&#39;num_trees&#39;, 10), (&#39;depth&#39;, 3), (&#39;criterion&#39;, &#39;gini&#39;), (&#39;substrategy&#39;, &#39;all&#39;)])\nF Score: 0.288417\nParam Grid: dict_items([(&#39;num_trees&#39;, 10), (&#39;depth&#39;, 5), (&#39;criterion&#39;, &#39;gini&#39;), (&#39;substrategy&#39;, &#39;sqrt&#39;)])\nF Score: 0.304559\nParam Grid: dict_items([(&#39;num_trees&#39;, 10), (&#39;depth&#39;, 5), (&#39;criterion&#39;, &#39;gini&#39;), (&#39;substrategy&#39;, &#39;all&#39;)])\nF Score: 0.296859\nParam Grid: dict_items([(&#39;num_trees&#39;, 10), (&#39;depth&#39;, 10), (&#39;criterion&#39;, &#39;gini&#39;), (&#39;substrategy&#39;, &#39;sqrt&#39;)])\nF Score: 0.323125\nParam Grid: dict_items([(&#39;num_trees&#39;, 10), (&#39;depth&#39;, 10), (&#39;criterion&#39;, &#39;gini&#39;), (&#39;substrategy&#39;, &#39;all&#39;)])\nF Score: 0.294743\nParam Grid: dict_items([(&#39;num_trees&#39;, 20), (&#39;depth&#39;, 3), (&#39;criterion&#39;, &#39;gini&#39;), (&#39;substrategy&#39;, &#39;sqrt&#39;)])\n</div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Cancelled","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#References\n\n - https://machinelearningmastery.com/k-fold-cross-validation/\n - https://www.analyticsvidhya.com/blog/2019/11/build-machine-learning-pipelines-pyspark/\n - https://medium.com/@junwan01/oversampling-and-undersampling-with-pyspark-5dbc25cdf253"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a92ae128-c361-4888-93bb-bad5875dcd1c"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"JN_Models_CV","dashboards":[{"elements":[{"elementNUID":"3c724a43-5463-4a4f-aeb0-232028f49412","guid":"08c988bc-b8a1-4c0f-8d22-983d8f3dfacd","resultIndex":null,"options":null,"position":{"x":0,"y":2,"height":2,"width":12,"z":null},"elementType":"command"},{"elementNUID":"90427fe0-b31f-493e-ad19-292a31643253","guid":"31c64de6-d5e8-4151-8925-fd951ec9bab1","resultIndex":null,"options":null,"position":{"x":0,"y":0,"height":2,"width":12,"z":null},"elementType":"command"},{"elementNUID":"866585bd-c1d7-4f01-804d-ae881d24d48b","guid":"4dc39a43-a780-4c11-a44b-a2293359ce85","resultIndex":null,"options":null,"position":{"x":0,"y":10,"height":2,"width":12,"z":null},"elementType":"command"},{"elementNUID":"f4ef8332-5fd7-4d3d-b86b-43cc45fa14c3","guid":"571eed1b-006f-43fb-be15-96ba1e5a9bed","resultIndex":null,"options":null,"position":{"x":0,"y":8,"height":2,"width":12,"z":null},"elementType":"command"},{"elementNUID":"32d92fe8-bde0-4daa-8c5f-1c887f9a0b3b","guid":"65b931a0-4404-47f0-8918-cf654e706161","resultIndex":null,"options":null,"position":{"x":0,"y":38,"height":2,"width":12,"z":null},"elementType":"command"},{"elementNUID":"da2e947b-e23a-4315-a962-d662b487ba60","guid":"92335cca-85aa-470f-9d9f-b738f52760a3","resultIndex":null,"options":null,"position":{"x":0,"y":4,"height":2,"width":12,"z":null},"elementType":"command"},{"elementNUID":"6e7922ea-536f-4a49-a76f-4e2f5487dd9c","guid":"92ea2884-ca90-4f0c-b001-ae4d4426e903","resultIndex":null,"options":null,"position":{"x":0,"y":6,"height":2,"width":12,"z":null},"elementType":"command"},{"elementNUID":"332541dc-d6d6-43b4-8544-6398c75d561f","guid":"b58d8c9d-e0b9-44a3-9b78-13318e3b64dc","resultIndex":null,"options":null,"position":{"x":0,"y":34,"height":2,"width":12,"z":null},"elementType":"command"},{"elementNUID":"a92ae128-c361-4888-93bb-bad5875dcd1c","guid":"b62aae48-b397-4ce0-96e1-3a3f9357ff38","resultIndex":null,"options":null,"position":{"x":0,"y":42,"height":4,"width":12,"z":null},"elementType":"command"}],"guid":"73604bee-d109-4d5c-a552-9f3c89ad7e3e","layoutOption":{"stack":true,"grid":true},"version":"DashboardViewV1","nuid":"c0b8d884-7500-4a24-a941-22443ebe70e8","origId":4070574709982890,"title":"Untitled","width":1440,"globalVars":{}}],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":4070574709982838}},"nbformat":4,"nbformat_minor":0}

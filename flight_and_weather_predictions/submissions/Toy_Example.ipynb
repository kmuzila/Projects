{"cells":[{"cell_type":"markdown","source":["# Toy Example\n\nThe following is a toy example using Logistic regression that has been applied and tested on our dataset. In this notebook, Logistic regression calculations are explained and how the algorithm is implemented. \n\nLogistic regression's representative equation is like linear regression with the key difference being the value output of a binary value instead of numeric value. Logistic models are usually implemented to predict the probability of a classification or outcome of an event. Logistic regression matches our use case of predicting the binary classification if a flight is delayed or not delayed. The algorithm also benefits of being interpretable which makes it a good choice for our toy example."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"652e8e27-d923-4add-a301-a6f22bddcfa3"}}},{"cell_type":"markdown","source":["The Logistic regression is represented by the following equation: \n\n$$\nf(X) = \\beta_{0} + \\sum_{j=1}^{p}[X_{j}\\beta_{j}]\n$$\n\nWhere X represents the input and Beta represents the weight."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"edbc0d7f-2d44-42d0-8b57-8798c204ce7d"}}},{"cell_type":"markdown","source":["Logistic regression, as a matrix, has a similar base as linear regression which is represented as:\n\n$$ \nh(x) = \\theta^Tx\n$$\n\nWhere x represents a vector of inputs and Theta represents a vector of weights."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f1b0602-c4b6-45d2-b059-7340d5bde508"}}},{"cell_type":"markdown","source":["We can rewrite the expression to represent multiple features using the above equations as the following:\n\n$$\nh(x) = \\theta_0 + \\theta_1x1 + \\theta_2x2 \\ldots\n$$"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ee4c95f3-3820-4f2a-8322-2d9b8d49193b"}}},{"cell_type":"markdown","source":["We then apply a sigmoid function that represents the conditional probability of logistic regression between outputs of 0 and 1. \n\n> The sigmoid function is: \n> \n> $$\nJ(t) = \\frac{1}{1+e^{-t}}\n> $$\n>\n> Which when applied to the linear regression base is:\n> \n> $$\nh(x) = J(\\theta^Tx)\n> $$\nWhere h(x) ranges from 0 to 1 in which both probabilities will add up to 1."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eb83d63f-b8c0-4956-acc1-3bc075f186da"}}},{"cell_type":"markdown","source":["The above equations can then be rewritten together as the following expression:\n\n$$h(x) = \\frac{1}{1+e^{-\\theta^TX}}$$"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e8d85de0-7b2d-4a76-95d3-e7cfabfd8f27"}}},{"cell_type":"markdown","source":["The purpose of the Logistic regression model is to fit with the data and to minimize the cost or loss function. We want to apply the cost or loss function in order to compute the error of the model and compare the model performance when predicting classification. \n\nWe can then apply a cost or loss function to the algorithm which can penalize the model if the model predicts an incorrect classification. The logistic loss can be calculated using:\n\n> The equation if the correct classification is one (y=1) and the model predicts zero\n> $$\n-log(1-h(x))\n> $$\n> \n> The equation if the correct classification is zero (y=0) and the model predicts one\n> $$\n-log(h(x))\n> $$"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d742fbb5-deb9-4965-bfad-2d43dc002a3d"}}},{"cell_type":"markdown","source":["The following is the squared loss function of Logistic regression which contains multiple local minima and alternatives such as hinge loss. Combining the above equations to create the complete cost function equation:\n\n$$\ncost(h(x),y)=-ylog(h(x))-log(1-h(x))(1-y)\n$$\n\nWhen the model predicts the label to be y=1 with the probability of 1, then the cost function is -log(1)=0 and the loss fuction is equal to 0 which indicates an ideal prediction. Similarly, when the model predicts the label to be y=0 with the probability of 1, then the cost function will be -log(1-1). When the model determines an incorrect prediction of P(y^{hat} : 0)=0.999 and its probability is P(y^{hat}=0.001), then the loss function will be -log(0.001) which is a larger error. The reason that we use 0.999 and 0.001 is because -log(0) approaches infinity. Therefore, the loss function will approach infinity, when the model prediction is y=0, as the correct prediction approaches the probability of 0."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b7bd0661-7a7b-4516-8815-89121e6f293d"}}},{"cell_type":"markdown","source":["We can compute the compute the loss of the whole training set by calculating the average over the cost of whole training set's loss with a vector of n parameters:\n\n$$\nM(\\theta) = \\frac{1}{n}\\sum_{i=1}^{n}[y^i log(h(x^i))+log(1-h(x^i))(1-y^i)]\n$$"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cc703f85-5137-4129-bde8-fc3a8b0c5050"}}},{"cell_type":"markdown","source":["The logistic regression weights cna be selected randomly and the cost function can be analyzed to determine if the new model has improved from the last. However, this can be inefficient. As an alternative, we can iteratively take the derivative of the cost function closer to 0 each time to obtain the slope to find its minimum, while staying cautious that we are moving towards the minimum and not its maximum. There are several other algorithms that take this approach, however, we will be using Gradient Descent for our purposes. \n\nIn Gradient Descent, the first-order derivative of the cost function provides the slope or gradient. Its step-size or learning rate is set by the user and remains constant for each step. The multiple iterations will eventually reach a minimum. \n\nThe following equation uses gradient descent to calculate the derivatives and minimize cost:\n\n$$\n\\frac{\\partial M(\\theta)}{\\partial\\theta_j} =  \\frac{1}{n}\\sum_{i=1}^{n}x^i_j(h(x^i)-y^i)\n$$"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"60999740-67bf-481e-8b6d-e96f3a0f0ae5"}}},{"cell_type":"markdown","source":["## Toy Example Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d37b49f0-555d-43df-bf65-eb223a7ad8b0"}}},{"cell_type":"code","source":["import datetime\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom pytz import timezone\nfrom pyspark import StorageLevel\nfrom pyspark.sql import DataFrameNaFunctions,Window\nfrom pyspark.sql.types import IntegerType,BooleanType,DateType,StringType,TimestampType\nfrom pyspark.sql.functions import col,isnan,isnull,when,count,lit,to_date,lpad,date_format,rpad,regexp_replace,concat,to_utc_timestamp,to_timestamp,countDistinct,unix_timestamp,row_number,when,percent_rank,year\nfrom pyspark.ml import Pipeline, PipelineModel\nfrom pyspark.ml.stat import Correlation\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder, StandardScaler, PCA, VectorSlicer, Imputer, MinMaxScaler\nfrom pyspark.ml.linalg import Vectors,DenseMatrix\nfrom pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"99261850-4d2f-4a6d-806e-568c62f78364"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["blob_container = \"tm30container\" # The name of your container created in https://portal.azure.com\nstorage_account = \"w261tm30\" # The name of your Storage account created in https://portal.azure.com\nsecret_scope = \"w261tm30\" # The name of the scope created in your local computer using the Databricks CLI\nsecret_key = \"tm30key\" # The name of the secret key created in your local computer using the Databricks CLI \nblob_url = f\"wasbs://{blob_container}@{storage_account}.blob.core.windows.net\"\nmount_path = \"/mnt/mids-w261\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"93d36eb6-63de-498d-9b81-375383157922"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["index_cols = ['UNIQUE_ID',\n              'FLIGHT_UTC_DATE',\n              'rank'\n             ]\n\ncat_cols = ['TIME_OF_DAY',\n            'MONTH',\n            'DAY_OF_WEEK',\n            'OP_UNIQUE_CARRIER',\n            'wnd_type',\n            'cig_ceil_is_qual',\n            'tmp_air_is_qual',\n            'slp_prs_is_qual',\n            'ga1_cov','ga1_cld',\n            'ga1_bs_ht_is_qual',\n            'wnd_spd_is_qual',\n            'ga1_cld_qual',\n            'dew_pnt_is_qual',\n            'ga1_cov_is_qual',\n            'aa1_is_qual',\n            'vis_dist_is_qual',\n            'ka1_temp',\n            'FLIGHT_ROUTE'\n           ]\n\ncont_cols = ['ELEVATION',\n             'wnd_dir_angle',\n             'wnd_spd_rate',\n             'cig_ceil_ht',\n             'vis_dist',\n             'tmp_air',\n             'dew_pnt_tmp',\n             'slp_prs',\n             'aa1_prd_quant_hr',\n             'aa1_dp',\n             'ga1_bs_ht'\n            ]\n\npred_cols = ['DEP_DEL15']"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bfda074d-0ac3-4ddd-819f-d83df4ca6b03"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def data_pull(df, time_window = 'full', date_col='FLIGHT_UTC_DATE'):\n    if time_window == '2019':\n        df = df.filter(year(col(date_col)) == 2019)\n    elif time_window == '2018':\n        df = df.filter(year(col(date_col)) == 2018)\n    elif time_window == '2017':\n        df = df.filter(year(col(date_col)) == 2017)\n    elif time_window == '2016':\n        df = df.filter(year(col(date_col)) == 2016) \n    elif time_window == '6m':\n        df = df.filter(col(date_col) < \"2015-07-01T00:00:00.000\")  \n    elif time_window == '3m':\n        df = df.filter(col(date_col) < \"2015-04-01T00:00:00.000\")\n    return df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"db07a10d-c734-4605-a6cd-4c6a4929a497"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def pre_pipeline(index_cols, cont_cols, cat_cols, pred_cols):\n    indexer = StringIndexer(inputCols=cat_cols, outputCols=[c+\"_idx\" for c in cat_cols]).setHandleInvalid(\"keep\")\n    encoder = OneHotEncoder(inputCols=[c+\"_idx\" for c in cat_cols], outputCols= [c+\"_OHE\" for c in cat_cols])\n    assembler_cat = VectorAssembler(inputCols= [x+\"_OHE\" for x in cat_cols], outputCol=\"cat_features\")\n    imputer = Imputer(inputCols=cont_cols, outputCols=cont_cols)\n    assembler_lab = StringIndexer(inputCol='DEP_DEL15', outputCol=\"label\")\n    return Pipeline(stages=[indexer, encoder, assembler_cat, imputer, assembler_lab])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5f618702-726e-4f99-8bd0-7b139d8e19aa"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["The function `sigmoid` is an implementation of the above sigmoid equation that we will apply to the output of the linear regression algorithm to produce a logistic regression model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"13c60e66-b9f7-4218-a74a-60ccc74255e0"}}},{"cell_type":"code","source":["def sigmoid(e):\n    return 1/(1 + np.exp(-e))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"483b03e1-8a60-4d75-bfcd-d8c4d3b0d9df"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["We implemented the cost function equation as the function `Loss`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"36eacbca-49e5-4f1e-a294-61195d9aea9e"}}},{"cell_type":"code","source":["def Loss(dataRDD, W): \n    augmented_data = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1]))\n    #### LOSS CAME FROM OTHER NB\n    loss = augmented_data.map(lambda x: x[1] * np.log(sigmoid(W.dot(x[0]))) + (1-x[1]) * np.log(sigmoid(W.dot(x[0])))).mean()\n    return loss"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d8ac50da-1765-4e7b-be85-4d43c87c0011"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["The gradient descent equation is translated into the `GDUpdate` function that is used to compute the model gradient and update model parameters each training step"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9c5cfd3a-13bf-40b5-8d0a-ccb2e0f7aa47"}}},{"cell_type":"code","source":["def GDUpdate(dataRDD, W, learningRate = 0.1):\n    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1])).cache()\n    grad = (augmentedData.map(lambda x: 2*(sigmoid(W.dot(x[0]))-x[1])*x[0]).mean())\n    new_model = W-(learningRate*grad)\n    return new_model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b851eca8-b98e-4255-9d8d-d488c2667953"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["The `GradientDescent` function encompasses all of the above functions and manages the model training process returning data from all model training steps for the train data, test data, and models."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f45d39cd-351c-4cdf-baa7-354b57860a00"}}},{"cell_type":"code","source":["def GradientDescent(trainRDD, testRDD, wInit, nSteps = 20, learningRate = 0.1, verbose = False):\n    train_history, test_history, model_history = [], [], []\n    model = wInit\n    for idx in range(nSteps): \n        model = GDUpdate(trainRDD,model,learningRate)\n        training_loss = Loss(trainRDD,model)\n        test_loss = Loss(testRDD,model)\n        train_history.append(training_loss)\n        test_history.append(test_loss)\n        model_history.append(model)\n    return train_history, test_history, model_history"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"84049300-a700-449e-8cd2-67c806fa7c89"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["test_pq = spark.read.parquet(f\"{blob_url}/2022-03-24_data_chkpt_PQ_full\")\n\ntest_pq = test_pq.na.replace('', None, 'wnd_type') \\\n                 .na.replace('', None, 'ga1_cld') \\\n                 .na.replace('', None, 'ga1_cov') \\\n                 .withColumn('wnd_dir_angle',col('wnd_dir_angle').cast(IntegerType())) \\\n                 .withColumn('ka1_temp', when(isnull('ka1_temp'), '0').when(col('ka1_temp') < 0, -1).otherwise('1')) \\\n                 .withColumn('FLIGHT_ROUTE', concat(col('ORIGIN'),lit(\"-\"),col('DEST')))\n\ndf_6m = data_pull(test_pq, time_window='6m', date_col='FLIGHT_UTC_DATE')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58bbcd64-ad7c-4483-a699-eeecdacbffc6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["pre_pipeline_test = pre_pipeline(index_cols, cont_cols, cat_cols, pred_cols)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e49093b0-33da-4d6d-bb43-a44de9dbde7c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["df_fit = pre_pipeline_test.fit(df_6m)\ndf_transform = df_fit.transform(df_6m)\ndf_select = df_transform.select('label','cat_features')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"97049407-8167-4200-b1e1-7b1c9716306a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["train,test = df_select.randomSplit([0.7,0.3],seed=2022)\ntrainRDD = train.sample(0.0002, 2022).rdd.map(lambda x: (x[1].toArray(),x[0])).cache()\ntestRDD = test.sample(0.0002, 2022).rdd.map(lambda x: (x[1].toArray(),x[0])).cache()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"77786461-a5bc-4305-973d-ff3681b4be6f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["trainRDD.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8fe2c55d-3f1f-4f1a-8dba-07a2cfb00451"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[40]: 450</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[40]: 450</div>"]}}],"execution_count":0},{"cell_type":"code","source":["testRDD.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"59a1a12d-fd74-4a10-a8af-1cf11dfa4071"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[41]: 189</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[41]: 189</div>"]}}],"execution_count":0},{"cell_type":"code","source":["wInit = np.insert(np.zeros(trainRDD.take(1)[0][0].shape[0]),0,trainRDD.map(lambda x: x[1]).mean())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"613b6eac-a172-4ff3-b43a-09f2ec21b93c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["model_results = GradientDescent(trainRDD, testRDD, wInit)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1dcf912b-665f-45d8-b2c9-a5fe52dee6e6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["df_pred = pd.DataFrame(model_results).T\ndf_pred"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8c286b71-631e-4d71-b68f-155eff1da013"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[46]: </div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[46]: </div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1.111493</td>\n      <td>-1.10947</td>\n      <td>[0.1428168070880142, -0.03214797868416597, -0....</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1.335307</td>\n      <td>-1.331895</td>\n      <td>[0.11921523406226187, -0.04561971930336953, -0...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-1.439474</td>\n      <td>-1.435026</td>\n      <td>[0.10880359075296206, -0.053562985977021776, -...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-1.492099</td>\n      <td>-1.486781</td>\n      <td>[0.10359213213328998, -0.05930822881277332, -0...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-1.519784</td>\n      <td>-1.513681</td>\n      <td>[0.1008071443671205, -0.06400990456814812, -0....</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-1.534673</td>\n      <td>-1.527834</td>\n      <td>[0.09924607031163524, -0.06816784298329132, -0...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>-1.542803</td>\n      <td>-1.535257</td>\n      <td>[0.09832670603410014, -0.0720237597434065, -0....</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>-1.54731</td>\n      <td>-1.539075</td>\n      <td>[0.09775060863627648, -0.07570148267411392, -0...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>-1.549862</td>\n      <td>-1.540952</td>\n      <td>[0.09736007695254821, -0.07926646177217157, -0...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>-1.551359</td>\n      <td>-1.541783</td>\n      <td>[0.09707040813379739, -0.08275391772752123, -0...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>-1.55229</td>\n      <td>-1.542056</td>\n      <td>[0.09683570619321068, -0.08618302758217608, -0...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>-1.552921</td>\n      <td>-1.542035</td>\n      <td>[0.09663098682354487, -0.08956433759557555, -0...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>-1.553398</td>\n      <td>-1.541866</td>\n      <td>[0.0964426183233307, -0.09290371666299263, -0....</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>-1.553799</td>\n      <td>-1.541626</td>\n      <td>[0.09626315721151045, -0.09620448875751703, -0...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>-1.554167</td>\n      <td>-1.541358</td>\n      <td>[0.09608854060843781, -0.09946859035268994, -0...</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>-1.554526</td>\n      <td>-1.541087</td>\n      <td>[0.09591655461320554, -0.10269720078037867, -0...</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>-1.554888</td>\n      <td>-1.540824</td>\n      <td>[0.09574599718515282, -0.10589108616483953, -0...</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>-1.555259</td>\n      <td>-1.540575</td>\n      <td>[0.09557622013313267, -0.1090507872507316, -0....</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>-1.555644</td>\n      <td>-1.540344</td>\n      <td>[0.09540687837905186, -0.11217672201389735, -0...</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>-1.556044</td>\n      <td>-1.540133</td>\n      <td>[0.09523779264748646, -0.11526924171149672, -0...</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1.111493</td>\n      <td>-1.10947</td>\n      <td>[0.1428168070880142, -0.03214797868416597, -0....</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1.335307</td>\n      <td>-1.331895</td>\n      <td>[0.11921523406226187, -0.04561971930336953, -0...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-1.439474</td>\n      <td>-1.435026</td>\n      <td>[0.10880359075296206, -0.053562985977021776, -...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-1.492099</td>\n      <td>-1.486781</td>\n      <td>[0.10359213213328998, -0.05930822881277332, -0...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-1.519784</td>\n      <td>-1.513681</td>\n      <td>[0.1008071443671205, -0.06400990456814812, -0....</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-1.534673</td>\n      <td>-1.527834</td>\n      <td>[0.09924607031163524, -0.06816784298329132, -0...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>-1.542803</td>\n      <td>-1.535257</td>\n      <td>[0.09832670603410014, -0.0720237597434065, -0....</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>-1.54731</td>\n      <td>-1.539075</td>\n      <td>[0.09775060863627648, -0.07570148267411392, -0...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>-1.549862</td>\n      <td>-1.540952</td>\n      <td>[0.09736007695254821, -0.07926646177217157, -0...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>-1.551359</td>\n      <td>-1.541783</td>\n      <td>[0.09707040813379739, -0.08275391772752123, -0...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>-1.55229</td>\n      <td>-1.542056</td>\n      <td>[0.09683570619321068, -0.08618302758217608, -0...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>-1.552921</td>\n      <td>-1.542035</td>\n      <td>[0.09663098682354487, -0.08956433759557555, -0...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>-1.553398</td>\n      <td>-1.541866</td>\n      <td>[0.0964426183233307, -0.09290371666299263, -0....</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>-1.553799</td>\n      <td>-1.541626</td>\n      <td>[0.09626315721151045, -0.09620448875751703, -0...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>-1.554167</td>\n      <td>-1.541358</td>\n      <td>[0.09608854060843781, -0.09946859035268994, -0...</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>-1.554526</td>\n      <td>-1.541087</td>\n      <td>[0.09591655461320554, -0.10269720078037867, -0...</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>-1.554888</td>\n      <td>-1.540824</td>\n      <td>[0.09574599718515282, -0.10589108616483953, -0...</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>-1.555259</td>\n      <td>-1.540575</td>\n      <td>[0.09557622013313267, -0.1090507872507316, -0....</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>-1.555644</td>\n      <td>-1.540344</td>\n      <td>[0.09540687837905186, -0.11217672201389735, -0...</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>-1.556044</td>\n      <td>-1.540133</td>\n      <td>[0.09523779264748646, -0.11526924171149672, -0...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Resources\n- https://towardsdatascience.com/building-a-logistic-regression-in-python-301d27367c24\n- [Week 6 Slides](https://docs.google.com/presentation/d/1VDPrWlJFgbsEIh5MhUoSbQcAKkgBve06lYhubqA48QE/edit#slide=id.gf4c8f735ab_0_200)\n- [W261 Homework 4](https://github.com/UCB-w261/main/blob/main/Assignments/HW4/docker/student/hw4_Workbook.ipynb)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3bb92ab5-c3df-49a7-8380-a6ff7c41c806"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Toy_Example","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":4070574710012102}},"nbformat":4,"nbformat_minor":0}
